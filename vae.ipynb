{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch; torch.manual_seed(0)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils\n",
    "import torch.distributions\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt; plt.rcParams['figure.dpi'] = 200\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# First 50 % of the data from NaI and rest 50 % is from PS.\n",
    "\n",
    "\n",
    "db_name = \"./Combined_spectrum_database_12_Sep_2023_RN_Ar-41_Co-60_Cs-137_I-131_K-40_Tl-208.db\"\n",
    "conn = sqlite3.connect(db_name)\n",
    "cursor = conn.cursor()\n",
    "x = []\n",
    "y = []\n",
    "try:\n",
    "    for i in range(100000):\n",
    "        query = f\"SELECT * FROM spec_data WHERE id = {i+1}\"\n",
    "        cursor.execute(query)\n",
    "        spectrum_data = cursor.fetchall()\n",
    "        x.append( np.array(json.loads(spectrum_data[0][2])) ) # Add NaI spectrum\n",
    "        y.append( np.array(list(json.loads(spectrum_data[0][1]).values()))/100 )  # Radionuclide Percentage\n",
    "        \n",
    "        \n",
    "finally:\n",
    "conn.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initializing hyperparameters\n",
    "batch_size = 1000\n",
    "number_of_batches = 100\n",
    "label_array_size = 6\n",
    "data_array_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for normalization \n",
    "def normlize_array(arr):\n",
    "    arr = arr/arr.sum()\n",
    "    return(arr)\n",
    "\n",
    "\n",
    "x_norm = []\n",
    "for i in x:\n",
    "    x_norm.append(normlize_array(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the data to [1000,100,1,1024] and splitting the data in train and test data\n",
    "\n",
    "\n",
    "data = torch.Tensor(x_norm).view(number_of_batches,batch_size,1,data_array_size)\n",
    "label = torch.Tensor(y).view(number_of_batches,batch_size,1,label_array_size)\n",
    "\n",
    "\n",
    "train_data, test_data, train_label, test_label = train_test_split(data, label, test_size=0.2,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalEncoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(VariationalEncoder, self).__init__()\n",
    "        #Dimensionality reduction fromn1024 to latent_dims \n",
    "        self.linear1 = nn.Linear(1024, 512)\n",
    "        self.linear_1 = nn.Linear(512, 256)\n",
    "        self.linear2 = nn.Linear(256, latent_dims)\n",
    "        self.linear3 = nn.Linear(256, latent_dims)\n",
    "        #self.dropout = nn.Dropout(0.2)\n",
    "        self.N = torch.distributions.Normal(0, 1)\n",
    "        self.kl = 0\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear_1(x)\n",
    "        #x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        # Extracting Probabiluty Distribution\n",
    "        mu =  self.linear2(x)\n",
    "        sigma = torch.exp(self.linear3(x))\n",
    "        # Z = Latent Vector\n",
    "        z = mu + sigma*self.N.sample(mu.shape)\n",
    "        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(latent_dims, 512)\n",
    "        # self.linear_1 = nn.Linear(256, 512)\n",
    "        self.linear2 = nn.Linear(512, 1024)\n",
    "        self.linear3 = nn.Linear(512, 6)\n",
    " def forward(self, z):\n",
    "        z_1 = F.relu(self.linear1(z))\n",
    "        decoded_spec = torch.sigmoid(self.linear2(z_1))\n",
    "        label = torch.softmax(self.linear3(z_1), dim=1)\n",
    "        return decoded_spec.reshape((-1, 1, 1, 1024)),    label.reshape((-1, 1, 1, 6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        self.encoder = VariationalEncoder(latent_dims)\n",
    "        self.decoder = Decoder(latent_dims)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat, y_hat = self.decoder(z)\n",
    "        return x_hat, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_epochs = 200\n",
    "\n",
    "\n",
    "def train(autoencoder, data,label ,epochs=number_of_epochs):\n",
    "    opt = torch.optim.Adam(autoencoder.parameters())\n",
    "    lm1 = [] #For getting loss value plots\n",
    "    for epoch in range(epochs):\n",
    "        print('epochs : ',epoch+1)\n",
    "        l1 = [] \n",
    "        l2 = []\n",
    "        \n",
    "        for x, y in zip(data,label):\n",
    "            x = torch.flatten(x, start_dim=1) \n",
    "            opt.zero_grad()\n",
    "            x_hat, y_hat = autoencoder(x)\n",
    "            x_hat = torch.flatten(x_hat, start_dim=1)\n",
    "            loss_m = ((x - x_hat)**2).sum()    # Reconstruction Loss\n",
    "            loss_kl = autoencoder.encoder.kl    # KL Divergence\n",
    "            # total loss  = Reconstruction LOss + KL Divergence\n",
    "            loss = loss_m + loss_kl \n",
    "            l1.append(loss_m)\n",
    "            l2.append(loss_kl)\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        \n",
    "lm1.append([torch.mean(torch.stack(l1)), torch.mean(torch.stack(l2))])\n",
    "    return autoencoder,x_hat,y_hat,lm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dims = 32\n",
    "vae = VariationalAutoencoder(latent_dims) # GPU\n",
    "vae,x_hat,y_hat = train(vae,train_data,train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
